{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the TensorFlow-Trained BERT Model to a PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    training_job_name\n",
    "except NameError:\n",
    "    print('+++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please wait for the Training notebook to finish.')\n",
    "    print('+++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous training_job_name: tensorflow-training-2020-09-26-18-44-25-975\n"
     ]
    }
   ],
   "source": [
    "print('Previous training_job_name: {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the TensorFlow-Trained Model from S3 to this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-085964654406/tensorflow-training-2020-09-26-18-44-25-975/output/model.tar.gz to models/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz $models_dir/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "try:\n",
    "    tar = tarfile.open('{}/model.tar.gz'.format(models_dir))\n",
    "    tar.extractall(path=models_dir)\n",
    "    tar.close()\n",
    "except Exception as e:\n",
    "    print('[ERROR] in tar operation: {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 825188\r\n",
      "drwxrwxr-x  7 ec2-user ec2-user      4096 Sep 26 19:20 .\r\n",
      "drwxrwxr-x 11 ec2-user ec2-user      4096 Sep 26 19:20 ..\r\n",
      "drwxr-xr-x  2 ec2-user ec2-user      4096 Sep 26 19:17 code\r\n",
      "drwxr-xr-x  2 ec2-user ec2-user      4096 Sep 26 19:17 metrics\r\n",
      "-rw-rw-r--  1 ec2-user ec2-user 844962197 Sep 26 19:18 model.tar.gz\r\n",
      "drwxr-xr-x  2 ec2-user ec2-user      4096 Sep 26 18:47 tensorboard\r\n",
      "drwxr-xr-x  3 ec2-user ec2-user      4096 Sep 26 18:47 tensorflow\r\n",
      "drwxr-xr-x  3 ec2-user ec2-user      4096 Sep 26 18:47 transformers\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al $models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 261692\r\n",
      "drwxr-xr-x 2 ec2-user ec2-user      4096 Sep 26 19:16 .\r\n",
      "drwxr-xr-x 3 ec2-user ec2-user      4096 Sep 26 18:47 ..\r\n",
      "-rw-r--r-- 1 ec2-user ec2-user      1358 Sep 26 19:16 config.json\r\n",
      "-rw-r--r-- 1 ec2-user ec2-user 267959068 Sep 26 19:16 tf_model.h5\r\n"
     ]
    }
   ],
   "source": [
    "transformer_model_dir = '{}/transformers/fine-tuned/'.format(models_dir)\n",
    "\n",
    "!ls -al $transformer_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"_num_labels\": 5,\r\n",
      "  \"activation\": \"gelu\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"DistilBertForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bad_words_ids\": null,\r\n",
      "  \"bos_token_id\": null,\r\n",
      "  \"decoder_start_token_id\": null,\r\n",
      "  \"dim\": 768,\r\n",
      "  \"do_sample\": false,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": false,\r\n",
      "  \"eos_token_id\": null,\r\n",
      "  \"finetuning_task\": null,\r\n",
      "  \"hidden_dim\": 3072,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\",\r\n",
      "    \"3\": \"LABEL_3\",\r\n",
      "    \"4\": \"LABEL_4\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"is_decoder\": false,\r\n",
      "  \"is_encoder_decoder\": false,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2,\r\n",
      "    \"LABEL_3\": 3,\r\n",
      "    \"LABEL_4\": 4\r\n",
      "  },\r\n",
      "  \"length_penalty\": 1.0,\r\n",
      "  \"max_length\": 20,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"min_length\": 0,\r\n",
      "  \"model_type\": \"distilbert\",\r\n",
      "  \"n_heads\": 12,\r\n",
      "  \"n_layers\": 6,\r\n",
      "  \"no_repeat_ngram_size\": 0,\r\n",
      "  \"num_beams\": 1,\r\n",
      "  \"num_return_sequences\": 1,\r\n",
      "  \"output_attentions\": false,\r\n",
      "  \"output_hidden_states\": false,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"prefix\": null,\r\n",
      "  \"pruned_heads\": {},\r\n",
      "  \"qa_dropout\": 0.1,\r\n",
      "  \"repetition_penalty\": 1.0,\r\n",
      "  \"seq_classif_dropout\": 0.2,\r\n",
      "  \"sinusoidal_pos_embds\": false,\r\n",
      "  \"task_specific_params\": null,\r\n",
      "  \"temperature\": 1.0,\r\n",
      "  \"tie_weights_\": true,\r\n",
      "  \"top_k\": 50,\r\n",
      "  \"top_p\": 1.0,\r\n",
      "  \"torchscript\": false,\r\n",
      "  \"use_bfloat16\": false,\r\n",
      "  \"vocab_size\": 30522\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "cat $transformer_model_dir/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the TensorFlow Model to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification # PyTorch version\n",
    "\n",
    "try:\n",
    "    loaded_pytorch_model = DistilBertForSequenceClassification.from_pretrained(transformer_model_dir,\n",
    "                                                                         id2label={\n",
    "                                                                           0: 1,\n",
    "                                                                           1: 2,\n",
    "                                                                           2: 3,\n",
    "                                                                           3: 4,\n",
    "                                                                           4: 5\n",
    "                                                                         },\n",
    "                                                                         label2id={\n",
    "                                                                           1: 0,\n",
    "                                                                           2: 1,\n",
    "                                                                           3: 2,\n",
    "                                                                           4: 3,\n",
    "                                                                           5: 4\n",
    "                                                                         },\n",
    "                                                                      from_tf=True)\n",
    "except Exception as e:\n",
    "    print('[ERROR] in loading model {}: '.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_distilbert.DistilBertForSequenceClassification'>\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(type(loaded_pytorch_model))\n",
    "print(loaded_pytorch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_device -1\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "inference_device = -1 # CPU: -1, GPU: 0\n",
    "print('inference_device {}'.format(inference_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "inference_pipeline = TextClassificationPipeline(model=loaded_pytorch_model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                                framework='pt',\n",
    "                                                device=inference_device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved it!  I will recommend this to everyone. [{'label': 5, 'score': 0.94240725}]\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"I loved it!  I will recommend this to everyone.\"\"\"\n",
    "print(review, inference_pipeline(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Really bad.  I hope they don't make this anymore. [{'label': 1, 'score': 0.9148224}]\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"Really bad.  I hope they don't make this anymore.\"\"\"\n",
    "print(review, inference_pipeline(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save The Transformer/PyTorch Model with `.save_pretrained()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_models_dir = './models/transformers/pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $pytorch_models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_pytorch_model.save_pretrained(pytorch_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 261588\r\n",
      "drwxrwxr-x 2 ec2-user ec2-user      4096 Sep 26 19:20 .\r\n",
      "drwxr-xr-x 4 ec2-user ec2-user      4096 Sep 26 19:20 ..\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user      1302 Sep 26 19:20 config.json\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 267852933 Sep 26 19:20 pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al $pytorch_models_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = DistilBertForSequenceClassification.from_pretrained(pytorch_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "inference_pipeline = None\n",
    "\n",
    "try: \n",
    "    inference_pipeline = TextClassificationPipeline(model=pytorch_model, \n",
    "                                                    tokenizer=tokenizer,\n",
    "                                                    framework='pt',\n",
    "                                                    device=inference_device) \n",
    "except Exception as e:\n",
    "    print('[ERROR] in creating inference_pipeline: {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved it!  I will recommend this to everyone. [{'label': 5, 'score': 0.94240725}]\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"I loved it!  I will recommend this to everyone.\"\"\"\n",
    "print(review, inference_pipeline(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Transformer/PyTorch Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_pytorch_model_name = 'pytorch_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-085964654406/models/transformer-pytorch/\n"
     ]
    }
   ],
   "source": [
    "transformer_pytorch_model_s3_uri = 's3://{}/models/transformer-pytorch/'.format(bucket)\n",
    "print(transformer_pytorch_model_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: models/transformers/pytorch/config.json to s3://sagemaker-us-west-2-085964654406/models/transformer-pytorch/config.json\n",
      "upload: models/transformers/pytorch/pytorch_model.bin to s3://sagemaker-us-west-2-085964654406/models/transformer-pytorch/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $pytorch_models_dir $transformer_pytorch_model_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-26 19:20:38       1302 config.json\r\n",
      "2020-09-26 19:20:38  267852933 pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $transformer_pytorch_model_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'transformer_pytorch_model_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store transformer_pytorch_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'transformer_pytorch_model_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store transformer_pytorch_model_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "auto_ml_job_name                                      -> 'automl-dm-26-16-00-25'\n",
      "autopilot_endpoint_name                               -> 'automl-dm-ep-26-16-21-49'\n",
      "autopilot_train_s3_uri                                -> 's3://sagemaker-us-west-2-085964654406/data/amazon\n",
      "balance_dataset                                       -> True\n",
      "experiment_name                                       -> 'Amazon-Customer-Reviews-BERT-Experiment-160114585\n",
      "ingest_create_athena_db_passed                        -> True\n",
      "ingest_create_athena_table_parquet_passed             -> True\n",
      "ingest_create_athena_table_tsv_passed                 -> True\n",
      "max_seq_length                                        -> 64\n",
      "prepare_trial_component_name                          -> 'TrialComponent-2020-09-26-184417-oahc'\n",
      "processed_test_data_s3_uri                            -> 's3://sagemaker-us-west-2-085964654406/sagemaker-s\n",
      "processed_train_data_s3_uri                           -> 's3://sagemaker-us-west-2-085964654406/sagemaker-s\n",
      "processed_validation_data_s3_uri                      -> 's3://sagemaker-us-west-2-085964654406/sagemaker-s\n",
      "raw_input_data_s3_uri                                 -> 's3://sagemaker-us-west-2-085964654406/amazon-revi\n",
      "s3_private_path_tsv                                   -> 's3://sagemaker-us-west-2-085964654406/amazon-revi\n",
      "s3_public_path_tsv                                    -> 's3://amazon-reviews-pds/tsv'\n",
      "setup_dependencies_passed                             -> True\n",
      "setup_iam_roles_passed                                -> True\n",
      "setup_instance_check_passed                           -> True\n",
      "setup_s3_bucket_passed                                -> True\n",
      "test_split_percentage                                 -> 0.05\n",
      "train_split_percentage                                -> 0.9\n",
      "training_job_debugger_artifacts_path                  -> 's3://sagemaker-us-west-2-085964654406/tensorflow-\n",
      "training_job_name                                     -> 'tensorflow-training-2020-09-26-18-44-25-975'\n",
      "transformer_pytorch_model_name                        -> 'pytorch_model.bin'\n",
      "transformer_pytorch_model_s3_uri                      -> 's3://sagemaker-us-west-2-085964654406/models/tran\n",
      "trial_name                                            -> 'trial-1601145857'\n",
      "validation_split_percentage                           -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
