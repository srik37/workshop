{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Models using Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/hpt.png\" width=\"90%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-REQUISITE: _You need to have succesfully run the notebooks in the `PREPARE`section before you continue with this notebook._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the S3 Location of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r processed_train_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    processed_train_data_s3_uri\n",
    "    print('[OK]')\n",
    "except NameError:\n",
    "    print('+++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please run the notebooks in the PREPARE section before you continue.')\n",
    "    print('+++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-train\n"
     ]
    }
   ],
   "source": [
    "print(processed_train_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r processed_validation_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    processed_validation_data_s3_uri\n",
    "    print('[OK]')    \n",
    "except NameError:\n",
    "    print('+++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please run the notebooks in the previous sections before you continue.')\n",
    "    print('+++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-validation\n"
     ]
    }
   ],
   "source": [
    "print(processed_validation_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    processed_test_data_s3_uri\n",
    "    print('[OK]')    \n",
    "except NameError:\n",
    "    print('+++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please run the notebooks in the previous sections before you continue.')\n",
    "    print('+++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-train\n",
      "2020-09-26 17:50:36     352881 part-algo-1-amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tfrecord\n",
      "2020-09-26 17:50:36      11912 part-algo-1-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "2020-09-26 17:49:11      10766 part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(processed_train_data_s3_uri)\n",
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-validation\n",
      "2020-09-26 17:50:36      19944 part-algo-1-amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tfrecord\n",
      "2020-09-26 17:50:36        699 part-algo-1-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "2020-09-26 17:49:11        716 part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(processed_validation_data_s3_uri)\n",
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-test\n",
      "2020-09-26 17:50:36      19970 part-algo-1-amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tfrecord\n",
      "2020-09-26 17:50:36        710 part-algo-1-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "2020-09-26 17:49:11        650 part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(processed_test_data_s3_uri)\n",
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\r\n",
      "---------------------------------- -------------------\r\n",
      "absl-py                            0.10.0\r\n",
      "alabaster                          0.7.12\r\n",
      "anaconda-client                    1.7.2\r\n",
      "anaconda-project                   0.8.3\r\n",
      "argh                               0.26.2\r\n",
      "asn1crypto                         1.3.0\r\n",
      "astor                              0.8.1\r\n",
      "astroid                            2.4.2\r\n",
      "astropy                            4.0.1.post1\r\n",
      "atomicwrites                       1.3.0\r\n",
      "attrs                              19.3.0\r\n",
      "Automat                            20.2.0\r\n",
      "autopep8                           1.4.4\r\n",
      "autovizwidget                      0.15.0\r\n",
      "awscli                             1.18.137\r\n",
      "awswrangler                        1.9.3\r\n",
      "Babel                              2.8.0\r\n",
      "backcall                           0.1.0\r\n",
      "backports.shutil-get-terminal-size 1.0.0\r\n",
      "bcrypt                             3.2.0\r\n",
      "beautifulsoup4                     4.8.2\r\n",
      "bitarray                           1.2.1\r\n",
      "bkcharts                           0.2\r\n",
      "bleach                             3.1.4\r\n",
      "bokeh                              2.0.1\r\n",
      "boto                               2.49.0\r\n",
      "boto3                              1.14.60\r\n",
      "botocore                           1.17.60\r\n",
      "Bottleneck                         1.3.2\r\n",
      "cached-property                    1.5.1\r\n",
      "cachetools                         4.1.1\r\n",
      "certifi                            2020.6.20\r\n",
      "cffi                               1.14.0\r\n",
      "chardet                            3.0.4\r\n",
      "click                              7.1.1\r\n",
      "cloudpickle                        1.3.0\r\n",
      "clyent                             1.2.2\r\n",
      "colorama                           0.4.3\r\n",
      "contextlib2                        0.6.0.post1\r\n",
      "cryptography                       2.8\r\n",
      "cycler                             0.10.0\r\n",
      "Cython                             0.29.15\r\n",
      "cytoolz                            0.10.1\r\n",
      "dask                               2.14.0\r\n",
      "dataclasses                        0.7\r\n",
      "decorator                          4.4.2\r\n",
      "defusedxml                         0.6.0\r\n",
      "diff-match-patch                   20181111\r\n",
      "distributed                        2.14.0\r\n",
      "distro                             1.5.0\r\n",
      "docker                             4.3.1\r\n",
      "docker-compose                     1.27.2\r\n",
      "dockerpty                          0.4.1\r\n",
      "docopt                             0.6.2\r\n",
      "docutils                           0.15.2\r\n",
      "entrypoints                        0.3\r\n",
      "enum-compat                        0.0.3\r\n",
      "enum34                             1.1.10\r\n",
      "environment-kernels                1.1.1\r\n",
      "et-xmlfile                         1.0.1\r\n",
      "fastcache                          1.1.0\r\n",
      "filelock                           3.0.12\r\n",
      "flake8                             3.7.9\r\n",
      "Flask                              1.1.1\r\n",
      "fsspec                             0.7.1\r\n",
      "future                             0.18.2\r\n",
      "gast                               0.2.2\r\n",
      "gevent                             1.4.0\r\n",
      "glob2                              0.7\r\n",
      "gmpy2                              2.0.8\r\n",
      "google-auth                        1.21.3\r\n",
      "google-auth-oauthlib               0.4.1\r\n",
      "google-pasta                       0.2.0\r\n",
      "greenlet                           0.4.15\r\n",
      "grpcio                             1.32.0\r\n",
      "h5py                               2.10.0\r\n",
      "hdijupyterutils                    0.15.0\r\n",
      "HeapDict                           1.0.1\r\n",
      "html5lib                           1.0.1\r\n",
      "hypothesis                         5.8.3\r\n",
      "idna                               2.9\r\n",
      "imageio                            2.8.0\r\n",
      "imagesize                          1.2.0\r\n",
      "importlib-metadata                 1.5.0\r\n",
      "intervaltree                       3.0.2\r\n",
      "ipykernel                          5.1.4\r\n",
      "ipyparallel                        6.2.4\r\n",
      "ipython                            7.13.0\r\n",
      "ipython-genutils                   0.2.0\r\n",
      "ipywidgets                         7.5.1\r\n",
      "isort                              4.3.21\r\n",
      "itsdangerous                       1.1.0\r\n",
      "jdcal                              1.4.1\r\n",
      "jedi                               0.15.2\r\n",
      "jeepney                            0.4.3\r\n",
      "Jinja2                             2.11.1\r\n",
      "jmespath                           0.9.4\r\n",
      "joblib                             0.14.1\r\n",
      "json5                              0.9.4\r\n",
      "jsonschema                         3.2.0\r\n",
      "jupyter                            1.0.0\r\n",
      "jupyter-client                     6.1.2\r\n",
      "jupyter-console                    6.1.0\r\n",
      "jupyter-core                       4.6.3\r\n",
      "jupyterlab                         1.2.6\r\n",
      "jupyterlab-server                  1.1.0\r\n",
      "Keras-Applications                 1.0.8\r\n",
      "Keras-Preprocessing                1.1.2\r\n",
      "keyring                            21.1.1\r\n",
      "kiwisolver                         1.1.0\r\n",
      "lazy-object-proxy                  1.4.3\r\n",
      "libarchive-c                       2.8\r\n",
      "lief                               0.9.0\r\n",
      "llvmlite                           0.31.0\r\n",
      "locket                             0.2.0\r\n",
      "lxml                               4.5.0\r\n",
      "Markdown                           3.2.2\r\n",
      "MarkupSafe                         1.1.1\r\n",
      "matplotlib                         3.1.3\r\n",
      "mccabe                             0.6.1\r\n",
      "mistune                            0.8.4\r\n",
      "mkl-fft                            1.0.15\r\n",
      "mkl-random                         1.1.0\r\n",
      "mkl-service                        2.3.0\r\n",
      "mock                               4.0.1\r\n",
      "more-itertools                     8.2.0\r\n",
      "mpmath                             1.1.0\r\n",
      "msgpack                            1.0.0\r\n",
      "multipledispatch                   0.6.0\r\n",
      "nb-conda                           2.2.1\r\n",
      "nb-conda-kernels                   2.2.3\r\n",
      "nbconvert                          5.6.1\r\n",
      "nbformat                           5.0.4\r\n",
      "networkx                           2.4\r\n",
      "nltk                               3.4.5\r\n",
      "nose                               1.3.7\r\n",
      "notebook                           6.0.3\r\n",
      "numba                              0.48.0\r\n",
      "numexpr                            2.7.1\r\n",
      "numpy                              1.18.1\r\n",
      "numpydoc                           0.9.2\r\n",
      "nvidia-ml-py                       10.418.84\r\n",
      "oauthlib                           3.1.0\r\n",
      "olefile                            0.46\r\n",
      "opencv-python                      4.2.0.32\r\n",
      "openpyxl                           3.0.3\r\n",
      "opt-einsum                         3.3.0\r\n",
      "packaging                          20.3\r\n",
      "pandas                             1.0.5\r\n",
      "pandocfilters                      1.4.2\r\n",
      "paramiko                           2.7.2\r\n",
      "parso                              0.5.2\r\n",
      "partd                              1.1.0\r\n",
      "path                               13.1.0\r\n",
      "pathlib2                           2.3.5\r\n",
      "pathtools                          0.1.2\r\n",
      "patsy                              0.5.1\r\n",
      "pep8                               1.7.1\r\n",
      "pexpect                            4.8.0\r\n",
      "pickleshare                        0.7.5\r\n",
      "Pillow                             7.1.2\r\n",
      "pip                                20.2.3\r\n",
      "pkginfo                            1.5.0.1\r\n",
      "plotly                             4.9.0\r\n",
      "pluggy                             0.13.1\r\n",
      "ply                                3.11\r\n",
      "prometheus-client                  0.7.1\r\n",
      "prompt-toolkit                     3.0.4\r\n",
      "protobuf                           3.13.0\r\n",
      "protobuf3-to-dict                  0.1.5\r\n",
      "psutil                             5.7.0\r\n",
      "psycopg2                           2.7.5\r\n",
      "psycopg2-binary                    2.8.6\r\n",
      "ptyprocess                         0.6.0\r\n",
      "py                                 1.8.1\r\n",
      "py4j                               0.10.7\r\n",
      "pyarrow                            1.0.1\r\n",
      "pyasn1                             0.4.8\r\n",
      "pyasn1-modules                     0.2.8\r\n",
      "PyAthena                           1.10.7\r\n",
      "pycodestyle                        2.5.0\r\n",
      "pycosat                            0.6.3\r\n",
      "pycparser                          2.20\r\n",
      "pycrypto                           2.6.1\r\n",
      "pycurl                             7.43.0.5\r\n",
      "pydocstyle                         4.0.1\r\n",
      "pyflakes                           2.1.1\r\n",
      "pygal                              2.4.0\r\n",
      "Pygments                           2.6.1\r\n",
      "pykerberos                         1.2.1\r\n",
      "pylint                             2.5.3\r\n",
      "PyMySQL                            0.10.1\r\n",
      "PyNaCl                             1.4.0\r\n",
      "pyodbc                             4.0.0-unsupported\r\n",
      "pyOpenSSL                          19.1.0\r\n",
      "pyparsing                          2.4.6\r\n",
      "pyrsistent                         0.16.0\r\n",
      "PySocks                            1.7.1\r\n",
      "pyspark                            2.3.4\r\n",
      "pytest                             5.4.1\r\n",
      "pytest-arraydiff                   0.3\r\n",
      "pytest-astropy                     0.8.0\r\n",
      "pytest-astropy-header              0.1.2\r\n",
      "pytest-doctestplus                 0.5.0\r\n",
      "pytest-openfiles                   0.4.0\r\n",
      "pytest-remotedata                  0.3.2\r\n",
      "python-dateutil                    2.8.1\r\n",
      "python-dotenv                      0.14.0\r\n",
      "python-jsonrpc-server              0.3.4\r\n",
      "python-language-server             0.31.9\r\n",
      "pytz                               2019.3\r\n",
      "PyWavelets                         1.1.1\r\n",
      "pyxdg                              0.26\r\n",
      "PyYAML                             5.3.1\r\n",
      "pyzmq                              18.1.1\r\n",
      "QDarkStyle                         2.8\r\n",
      "QtAwesome                          0.7.0\r\n",
      "qtconsole                          4.7.2\r\n",
      "QtPy                               1.9.0\r\n",
      "regex                              2020.7.14\r\n",
      "requests                           2.23.0\r\n",
      "requests-kerberos                  0.12.0\r\n",
      "requests-oauthlib                  1.3.0\r\n",
      "retrying                           1.3.3\r\n",
      "rope                               0.16.0\r\n",
      "rsa                                4.5\r\n",
      "Rtree                              0.9.4\r\n",
      "ruamel-yaml                        0.15.87\r\n",
      "s3fs                               0.4.0\r\n",
      "s3transfer                         0.3.3\r\n",
      "sacremoses                         0.0.43\r\n",
      "sagemaker                          2.9.2\r\n",
      "sagemaker-experiments              0.1.24\r\n",
      "sagemaker-pyspark                  1.4.0\r\n",
      "scikit-image                       0.16.2\r\n",
      "scikit-learn                       0.23.1\r\n",
      "scipy                              1.4.1\r\n",
      "seaborn                            0.10.0\r\n",
      "SecretStorage                      3.1.2\r\n",
      "Send2Trash                         1.5.0\r\n",
      "sentencepiece                      0.1.91\r\n",
      "setuptools                         46.1.3.post20200330\r\n",
      "simplegeneric                      0.8.1\r\n",
      "singledispatch                     3.4.0.3\r\n",
      "six                                1.14.0\r\n",
      "smdebug                            0.9.3\r\n",
      "smdebug-rulesconfig                0.1.5\r\n",
      "snowballstemmer                    2.0.0\r\n",
      "sortedcollections                  1.1.2\r\n",
      "sortedcontainers                   2.1.0\r\n",
      "soupsieve                          2.0\r\n",
      "sparkmagic                         0.15.0\r\n",
      "Sphinx                             3.0.4\r\n",
      "sphinxcontrib-applehelp            1.0.2\r\n",
      "sphinxcontrib-devhelp              1.0.2\r\n",
      "sphinxcontrib-htmlhelp             1.0.3\r\n",
      "sphinxcontrib-jsmath               1.0.1\r\n",
      "sphinxcontrib-qthelp               1.0.3\r\n",
      "sphinxcontrib-serializinghtml      1.1.4\r\n",
      "sphinxcontrib-websupport           1.2.1\r\n",
      "spyder                             4.1.2\r\n",
      "spyder-kernels                     1.9.0\r\n",
      "SQLAlchemy                         1.3.13\r\n",
      "sqlalchemy-redshift                0.8.1\r\n",
      "statsmodels                        0.11.0\r\n",
      "stepfunctions                      2.0.0rc1\r\n",
      "sympy                              1.5.1\r\n",
      "tables                             3.6.1\r\n",
      "tblib                              1.6.0\r\n",
      "tenacity                           6.2.0\r\n",
      "tensorboard                        2.1.1\r\n",
      "tensorflow                         2.1.0\r\n",
      "tensorflow-estimator               2.1.0\r\n",
      "termcolor                          1.1.0\r\n",
      "terminado                          0.8.3\r\n",
      "testpath                           0.4.4\r\n",
      "texttable                          1.6.3\r\n",
      "threadpoolctl                      2.1.0\r\n",
      "tokenizers                         0.5.2\r\n",
      "toml                               0.10.1\r\n",
      "toolz                              0.10.0\r\n",
      "torch                              1.5.0\r\n",
      "torch-model-archiver               0.1.1\r\n",
      "torchserve                         0.1.1\r\n",
      "tornado                            6.0.4\r\n",
      "tqdm                               4.44.1\r\n",
      "traitlets                          4.3.3\r\n",
      "transformers                       2.8.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typed-ast                          1.4.1\r\n",
      "typing-extensions                  3.7.4.1\r\n",
      "ujson                              1.35\r\n",
      "unicodecsv                         0.14.1\r\n",
      "urllib3                            1.25.8\r\n",
      "watchdog                           0.10.2\r\n",
      "wcwidth                            0.1.9\r\n",
      "webencodings                       0.5.1\r\n",
      "websocket-client                   0.57.0\r\n",
      "Werkzeug                           1.0.1\r\n",
      "wheel                              0.34.2\r\n",
      "widgetsnbextension                 3.5.1\r\n",
      "wrapt                              1.12.1\r\n",
      "wurlitzer                          2.0.0\r\n",
      "xlrd                               1.2.0\r\n",
      "XlsxWriter                         1.2.8\r\n",
      "xlwt                               1.3.0\r\n",
      "yapf                               0.28.0\r\n",
      "zict                               2.0.0\r\n",
      "zipp                               2.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-train', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-validation', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-test', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "s3_input_train_data = TrainingInput(s3_data=processed_train_data_s3_uri, \n",
    "                                    distribution='ShardedByS3Key') \n",
    "s3_input_validation_data = TrainingInput(s3_data=processed_validation_data_s3_uri, \n",
    "                                         distribution='ShardedByS3Key')\n",
    "s3_input_test_data = TrainingInput(s3_data=processed_test_data_s3_uri, \n",
    "                                   distribution='ShardedByS3Key')\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import time\r\n",
      "import random\r\n",
      "import pandas as pd\r\n",
      "from glob import glob\r\n",
      "import pprint\r\n",
      "import argparse\r\n",
      "import json\r\n",
      "import subprocess\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.1.0.1.0.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'smdebug==0.9.3'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'matplotlib==3.2.1'])\r\n",
      "from transformers import DistilBertTokenizer\r\n",
      "from transformers import TFDistilBertForSequenceClassification\r\n",
      "from transformers import TextClassificationPipeline\r\n",
      "from transformers.configuration_distilbert import DistilBertConfig\r\n",
      "from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
      "from tensorflow.keras.models import load_model\r\n",
      "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\r\n",
      "\r\n",
      "\r\n",
      "CLASSES = [1, 2, 3, 4, 5]\r\n",
      "\r\n",
      "\r\n",
      "def select_data_and_label_from_record(record):\r\n",
      "    x = {\r\n",
      "        'input_ids': record['input_ids'],\r\n",
      "        'input_mask': record['input_mask'],\r\n",
      "        'segment_ids': record['segment_ids']\r\n",
      "    }\r\n",
      "\r\n",
      "    y = record['label_ids']\r\n",
      "\r\n",
      "    return (x, y)\r\n",
      "\r\n",
      "\r\n",
      "def file_based_input_dataset_builder(channel,\r\n",
      "                                     input_filenames,\r\n",
      "                                     pipe_mode,\r\n",
      "                                     is_training,\r\n",
      "                                     drop_remainder,\r\n",
      "                                     batch_size,\r\n",
      "                                     epochs,\r\n",
      "                                     steps_per_epoch,\r\n",
      "                                     max_seq_length):\r\n",
      "\r\n",
      "    # For training, we want a lot of parallel reading and shuffling.\r\n",
      "    # For eval, we want no shuffling and parallel reading doesn't matter.\r\n",
      "\r\n",
      "    if pipe_mode:\r\n",
      "        print('***** Using pipe_mode with channel {}'.format(channel))\r\n",
      "        from sagemaker_tensorflow import PipeModeDataset\r\n",
      "        dataset = PipeModeDataset(channel=channel,\r\n",
      "                                  record_format='TFRecord')\r\n",
      "    else:\r\n",
      "        print('***** Using input_filenames {}'.format(input_filenames))\r\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\r\n",
      "\r\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch * 100)\r\n",
      "#    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
      "\r\n",
      "    name_to_features = {\r\n",
      "      \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\r\n",
      "    }\r\n",
      "\r\n",
      "    def _decode_record(record, name_to_features):\r\n",
      "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\r\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\r\n",
      "        # TODO:  wip/bert/bert_attention_head_view/train.py\r\n",
      "        # Convert input_ids into input_tokens with DistilBert vocabulary \r\n",
      "        #  if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):\r\n",
      "        #    hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\r\n",
      "        return record\r\n",
      "    \r\n",
      "    dataset = dataset.apply(\r\n",
      "        tf.data.experimental.map_and_batch(\r\n",
      "          lambda record: _decode_record(record, name_to_features),\r\n",
      "          batch_size=batch_size,\r\n",
      "          drop_remainder=drop_remainder,\r\n",
      "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\r\n",
      "\r\n",
      "#    dataset.cache()\r\n",
      "\r\n",
      "    dataset = dataset.shuffle(buffer_size=1000,\r\n",
      "                              reshuffle_each_iteration=True)\r\n",
      "\r\n",
      "    row_count = 0\r\n",
      "    print('**************** {} *****************'.format(channel))\r\n",
      "    for row in dataset.as_numpy_iterator():\r\n",
      "        print(row)\r\n",
      "        if row_count == 5:\r\n",
      "            break\r\n",
      "        row_count = row_count + 1\r\n",
      "\r\n",
      "    return dataset\r\n",
      "\r\n",
      "\r\n",
      "def load_checkpoint_model(checkpoint_path):\r\n",
      "    import glob\r\n",
      "    import os\r\n",
      "    \r\n",
      "    glob_pattern = os.path.join(checkpoint_path, '*.h5')\r\n",
      "    print('glob pattern {}'.format(glob_pattern))\r\n",
      "\r\n",
      "    list_of_checkpoint_files = glob.glob(glob_pattern)\r\n",
      "    print('List of checkpoint files {}'.format(list_of_checkpoint_files))\r\n",
      "    \r\n",
      "    latest_checkpoint_file = max(list_of_checkpoint_files)\r\n",
      "    print('Latest checkpoint file {}'.format(latest_checkpoint_file))\r\n",
      "\r\n",
      "    initial_epoch_number_str = latest_checkpoint_file.rsplit('_', 1)[-1].split('.h5')[0]\r\n",
      "    initial_epoch_number = int(initial_epoch_number_str)\r\n",
      "\r\n",
      "    loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\r\n",
      "                                               latest_checkpoint_file,\r\n",
      "                                               config=config)\r\n",
      "\r\n",
      "    print('loaded_model {}'.format(loaded_model))\r\n",
      "    print('initial_epoch_number {}'.format(initial_epoch_number))\r\n",
      "    \r\n",
      "    return loaded_model, initial_epoch_number\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument('--train_data', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CHANNEL_TRAIN'])\r\n",
      "    parser.add_argument('--validation_data', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CHANNEL_VALIDATION'])\r\n",
      "    parser.add_argument('--test_data',\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_CHANNEL_TEST'])\r\n",
      "    parser.add_argument('--output_dir',\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_OUTPUT_DIR'])\r\n",
      "    parser.add_argument('--hosts', \r\n",
      "                        type=list, \r\n",
      "                        default=json.loads(os.environ['SM_HOSTS']))\r\n",
      "    parser.add_argument('--current_host', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CURRENT_HOST'])    \r\n",
      "    parser.add_argument('--num_gpus', \r\n",
      "                        type=int, \r\n",
      "                        default=os.environ['SM_NUM_GPUS'])\r\n",
      "    parser.add_argument('--checkpoint_base_path', \r\n",
      "                        type=str, \r\n",
      "                        default='/opt/ml/checkpoints')\r\n",
      "    parser.add_argument('--use_xla',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--use_amp',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--max_seq_length',\r\n",
      "                        type=int,\r\n",
      "                        default=64)\r\n",
      "    parser.add_argument('--train_batch_size',\r\n",
      "                        type=int,\r\n",
      "                        default=128)\r\n",
      "    parser.add_argument('--validation_batch_size',\r\n",
      "                        type=int,\r\n",
      "                        default=256)\r\n",
      "    parser.add_argument('--test_batch_size',\r\n",
      "                        type=int,\r\n",
      "                        default=256)\r\n",
      "    parser.add_argument('--epochs',\r\n",
      "                        type=int,\r\n",
      "                        default=2)\r\n",
      "    parser.add_argument('--learning_rate',\r\n",
      "                        type=float,\r\n",
      "                        default=0.00003)\r\n",
      "    parser.add_argument('--epsilon',\r\n",
      "                        type=float,\r\n",
      "                        default=0.00000001)\r\n",
      "    parser.add_argument('--train_steps_per_epoch',\r\n",
      "                        type=int,\r\n",
      "                        default=None)\r\n",
      "    parser.add_argument('--validation_steps',\r\n",
      "                        type=int,\r\n",
      "                        default=None)\r\n",
      "    parser.add_argument('--test_steps',\r\n",
      "                        type=int,\r\n",
      "                        default=None)\r\n",
      "    parser.add_argument('--freeze_bert_layer',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--enable_sagemaker_debugger',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--run_validation',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)    \r\n",
      "    parser.add_argument('--run_test',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)    \r\n",
      "    parser.add_argument('--run_sample_predictions',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--enable_tensorboard',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)        \r\n",
      "    parser.add_argument('--enable_checkpointing',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)    \r\n",
      "    parser.add_argument('--output_data_dir', # This is unused\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_OUTPUT_DATA_DIR'])\r\n",
      "    \r\n",
      "    # This points to the S3 location - this should not be used by our code\r\n",
      "    # We should use /opt/ml/model/ instead\r\n",
      "    # parser.add_argument('--model_dir', \r\n",
      "    #                     type=str, \r\n",
      "    #                     default=os.environ['SM_MODEL_DIR'])\r\n",
      "     \r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    print(\"Args:\") \r\n",
      "    print(args)\r\n",
      "    \r\n",
      "    env_var = os.environ \r\n",
      "    print(\"Environment Variables:\") \r\n",
      "    pprint.pprint(dict(env_var), width = 1) \r\n",
      "\r\n",
      "    print('SM_TRAINING_ENV {}'.format(env_var['SM_TRAINING_ENV']))\r\n",
      "    sm_training_env_json = json.loads(env_var['SM_TRAINING_ENV'])\r\n",
      "    is_master = sm_training_env_json['is_master']\r\n",
      "    print('is_master {}'.format(is_master))\r\n",
      "    \r\n",
      "    train_data = args.train_data\r\n",
      "    print('train_data {}'.format(train_data))\r\n",
      "    validation_data = args.validation_data\r\n",
      "    print('validation_data {}'.format(validation_data))\r\n",
      "    test_data = args.test_data\r\n",
      "    print('test_data {}'.format(test_data))    \r\n",
      "    local_model_dir = os.environ['SM_MODEL_DIR']\r\n",
      "    output_dir = args.output_dir\r\n",
      "    print('output_dir {}'.format(output_dir))    \r\n",
      "    hosts = args.hosts\r\n",
      "    print('hosts {}'.format(hosts))    \r\n",
      "    current_host = args.current_host\r\n",
      "    print('current_host {}'.format(current_host))    \r\n",
      "    num_gpus = args.num_gpus\r\n",
      "    print('num_gpus {}'.format(num_gpus))\r\n",
      "    job_name = os.environ['SAGEMAKER_JOB_NAME']\r\n",
      "    print('job_name {}'.format(job_name))    \r\n",
      "    use_xla = args.use_xla\r\n",
      "    print('use_xla {}'.format(use_xla))    \r\n",
      "    use_amp = args.use_amp\r\n",
      "    print('use_amp {}'.format(use_amp))    \r\n",
      "    max_seq_length = args.max_seq_length\r\n",
      "    print('max_seq_length {}'.format(max_seq_length))    \r\n",
      "    train_batch_size = args.train_batch_size\r\n",
      "    print('train_batch_size {}'.format(train_batch_size))    \r\n",
      "    validation_batch_size = args.validation_batch_size\r\n",
      "    print('validation_batch_size {}'.format(validation_batch_size))    \r\n",
      "    test_batch_size = args.test_batch_size\r\n",
      "    print('test_batch_size {}'.format(test_batch_size))    \r\n",
      "    epochs = args.epochs\r\n",
      "    print('epochs {}'.format(epochs))    \r\n",
      "    learning_rate = args.learning_rate\r\n",
      "    print('learning_rate {}'.format(learning_rate))    \r\n",
      "    epsilon = args.epsilon\r\n",
      "    print('epsilon {}'.format(epsilon))    \r\n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\r\n",
      "    print('train_steps_per_epoch {}'.format(train_steps_per_epoch))    \r\n",
      "    validation_steps = args.validation_steps\r\n",
      "    print('validation_steps {}'.format(validation_steps))    \r\n",
      "    test_steps = args.test_steps\r\n",
      "    print('test_steps {}'.format(test_steps))    \r\n",
      "    freeze_bert_layer = args.freeze_bert_layer\r\n",
      "    print('freeze_bert_layer {}'.format(freeze_bert_layer))    \r\n",
      "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\r\n",
      "    print('enable_sagemaker_debugger {}'.format(enable_sagemaker_debugger))    \r\n",
      "    run_validation = args.run_validation\r\n",
      "    print('run_validation {}'.format(run_validation))    \r\n",
      "    run_test = args.run_test\r\n",
      "    print('run_test {}'.format(run_test))    \r\n",
      "    run_sample_predictions = args.run_sample_predictions\r\n",
      "    print('run_sample_predictions {}'.format(run_sample_predictions))\r\n",
      "    enable_tensorboard = args.enable_tensorboard\r\n",
      "    print('enable_tensorboard {}'.format(enable_tensorboard))       \r\n",
      "    enable_checkpointing = args.enable_checkpointing\r\n",
      "    print('enable_checkpointing {}'.format(enable_checkpointing))    \r\n",
      "\r\n",
      "    checkpoint_base_path = args.checkpoint_base_path\r\n",
      "    print('checkpoint_base_path {}'.format(checkpoint_base_path))\r\n",
      "\r\n",
      "    if is_master:\r\n",
      "        checkpoint_path = checkpoint_base_path\r\n",
      "    else:\r\n",
      "        checkpoint_path = '/tmp/checkpoints'        \r\n",
      "    print('checkpoint_path {}'.format(checkpoint_path))\r\n",
      "    \r\n",
      "    # Determine if PipeMode is enabled \r\n",
      "    pipe_mode_str = os.environ.get('SM_INPUT_DATA_CONFIG', '')\r\n",
      "    pipe_mode = (pipe_mode_str.find('Pipe') >= 0)\r\n",
      "    print('Using pipe_mode: {}'.format(pipe_mode))\r\n",
      " \r\n",
      "    # Model Output \r\n",
      "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, 'transformers/fine-tuned/')\r\n",
      "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=True)\r\n",
      "\r\n",
      "    # SavedModel Output\r\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir, 'tensorflow/saved_model/0')\r\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\r\n",
      "\r\n",
      "    # Tensorboard Logs \r\n",
      "    tensorboard_logs_path = os.path.join(local_model_dir, 'tensorboard/')\r\n",
      "    os.makedirs(tensorboard_logs_path, exist_ok=True)\r\n",
      "\r\n",
      "    # Commented out due to incompatibility with transformers library (possibly)\r\n",
      "    # Set the global precision mixed_precision policy to \"mixed_float16\"    \r\n",
      "#    mixed_precision_policy = 'mixed_float16'\r\n",
      "#    print('Mixed precision policy {}'.format(mixed_precision_policy))\r\n",
      "#    policy = mixed_precision.Policy(mixed_precision_policy)\r\n",
      "#    mixed_precision.set_policy(policy)    \r\n",
      "    \r\n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\r\n",
      "    # Comment out when using smdebug as smdebug does not support MultiWorkerMirroredStrategy() as of smdebug 0.8.0\r\n",
      "    #distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n",
      "    with distributed_strategy.scope():\r\n",
      "        tf.config.optimizer.set_jit(use_xla)\r\n",
      "        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": use_amp})\r\n",
      "\r\n",
      "        train_data_filenames = glob(os.path.join(train_data, '*.tfrecord'))\r\n",
      "        print('train_data_filenames {}'.format(train_data_filenames))\r\n",
      "        train_dataset = file_based_input_dataset_builder(\r\n",
      "            channel='train',\r\n",
      "            input_filenames=train_data_filenames,\r\n",
      "            pipe_mode=pipe_mode,\r\n",
      "            is_training=True,\r\n",
      "            drop_remainder=False,\r\n",
      "            batch_size=train_batch_size,\r\n",
      "            epochs=epochs,\r\n",
      "            steps_per_epoch=train_steps_per_epoch,\r\n",
      "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "        tokenizer = None\r\n",
      "        config = None\r\n",
      "        model = None\r\n",
      "\r\n",
      "        # This is required when launching many instances at once...  the urllib request seems to get denied periodically\r\n",
      "        successful_download = False\r\n",
      "        retries = 0\r\n",
      "        while (retries < 5 and not successful_download):\r\n",
      "            try:\r\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "                config = DistilBertConfig.from_pretrained('distilbert-base-uncased',\r\n",
      "                                                          num_labels=len(CLASSES))\r\n",
      "                model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\r\n",
      "                                                                              config=config)\r\n",
      "                successful_download = True\r\n",
      "                print('Sucessfully downloaded after {} retries.'.format(retries))\r\n",
      "            except:\r\n",
      "                retries = retries + 1\r\n",
      "                random_sleep = random.randint(1, 30)\r\n",
      "                print('Retry #{}.  Sleeping for {} seconds'.format(retries, random_sleep))\r\n",
      "                time.sleep(random_sleep)\r\n",
      "\r\n",
      "        callbacks = []\r\n",
      "\r\n",
      "        initial_epoch_number = 0 \r\n",
      "\r\n",
      "        if enable_checkpointing:\r\n",
      "            print('***** Checkpoint enabled *****')\r\n",
      "            \r\n",
      "            os.makedirs(checkpoint_path, exist_ok=True)        \r\n",
      "            if os.listdir(checkpoint_path):\r\n",
      "                print('***** Found checkpoint *****')\r\n",
      "                print(checkpoint_path)\r\n",
      "                model, initial_epoch_number = load_checkpoint_model(checkpoint_path)\r\n",
      "                print('***** Using checkpoint model {} *****'.format(model))\r\n",
      "                \r\n",
      "            checkpoint_callback = ModelCheckpoint(\r\n",
      "                    filepath=os.path.join(checkpoint_path, 'tf_model_{epoch:05d}.h5'),\r\n",
      "                    save_weights_only=False,\r\n",
      "                    verbose=1,\r\n",
      "                    monitor='val_accuracy')\r\n",
      "            print('*** CHECKPOINT CALLBACK {} ***'.format(checkpoint_callback))\r\n",
      "            callbacks.append(checkpoint_callback)\r\n",
      "\r\n",
      "        if not tokenizer or not model or not config:\r\n",
      "            print('Not properly initialized...')\r\n",
      "\r\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\r\n",
      "        print('** use_amp {}'.format(use_amp))        \r\n",
      "        if use_amp:\r\n",
      "            # loss scaling is currently required when using mixed precision\r\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, 'dynamic')\r\n",
      "\r\n",
      "        print('enable_sagemaker_debugger {}'.format(enable_sagemaker_debugger))\r\n",
      "        if enable_sagemaker_debugger:\r\n",
      "            print('*** DEBUGGING ***')\r\n",
      "            import smdebug.tensorflow as smd\r\n",
      "            # This assumes that we specified debugger_hook_config\r\n",
      "            debugger_callback = smd.KerasHook.create_from_json_file()\r\n",
      "            print('*** DEBUGGER CALLBACK {} ***'.format(debugger_callback))            \r\n",
      "            callbacks.append(debugger_callback)\r\n",
      "            optimizer = debugger_callback.wrap_optimizer(optimizer)\r\n",
      "\r\n",
      "        if enable_tensorboard:            \r\n",
      "            tensorboard_callback = tf.keras.callbacks.TensorBoard(\r\n",
      "                                                        log_dir=tensorboard_logs_path)\r\n",
      "            print('*** TENSORBOARD CALLBACK {} ***'.format(tensorboard_callback))\r\n",
      "            callbacks.append(tensorboard_callback)\r\n",
      "  \r\n",
      "        print('*** OPTIMIZER {} ***'.format(optimizer))\r\n",
      "        \r\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\r\n",
      "\r\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n",
      "        print('Compiled model {}'.format(model))          \r\n",
      "        model.layers[0].trainable = not freeze_bert_layer\r\n",
      "        print(model.summary())\r\n",
      "\r\n",
      "        if run_validation:\r\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, '*.tfrecord'))\r\n",
      "            print('validation_data_filenames {}'.format(validation_data_filenames))\r\n",
      "            validation_dataset = file_based_input_dataset_builder(\r\n",
      "                channel='validation',\r\n",
      "                input_filenames=validation_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=False,\r\n",
      "                drop_remainder=False,\r\n",
      "                batch_size=validation_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=validation_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "            \r\n",
      "            print('Starting Training and Validation...')\r\n",
      "            validation_dataset = validation_dataset.take(validation_steps)\r\n",
      "            train_and_validation_history = model.fit(train_dataset,\r\n",
      "                                                     shuffle=True,\r\n",
      "                                                     epochs=epochs,\r\n",
      "                                                     initial_epoch=initial_epoch_number,\r\n",
      "                                                     steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                                     validation_data=validation_dataset,\r\n",
      "                                                     validation_steps=validation_steps,\r\n",
      "                                                     callbacks=callbacks)                                \r\n",
      "            print(train_and_validation_history)\r\n",
      "        else: # Not running validation\r\n",
      "            print('Starting Training (Without Validation)...')\r\n",
      "            train_history = model.fit(train_dataset,\r\n",
      "                                      shuffle=True,\r\n",
      "                                      epochs=epochs,\r\n",
      "                                      initial_epoch=initial_epoch_number,\r\n",
      "                                      steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                      callbacks=callbacks)                \r\n",
      "            print(train_history)\r\n",
      "\r\n",
      "        if run_test:\r\n",
      "            test_data_filenames = glob(os.path.join(test_data, '*.tfrecord'))\r\n",
      "            print('test_data_filenames {}'.format(test_data_filenames))\r\n",
      "            test_dataset = file_based_input_dataset_builder(\r\n",
      "                channel='test',\r\n",
      "                input_filenames=test_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=False,\r\n",
      "                drop_remainder=False,\r\n",
      "                batch_size=test_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=test_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "            print('Starting test...')\r\n",
      "            test_history = model.evaluate(test_dataset,\r\n",
      "                                          steps=test_steps,\r\n",
      "                                          callbacks=callbacks)\r\n",
      "                                 \r\n",
      "            print('Test history {}'.format(test_history))\r\n",
      "            \r\n",
      "        # Save the Fine-Yuned Transformers Model as a New \"Pre-Trained\" Model\r\n",
      "        print('transformer_fine_tuned_model_path {}'.format(transformer_fine_tuned_model_path))   \r\n",
      "        model.save_pretrained(transformer_fine_tuned_model_path)\r\n",
      "\r\n",
      "        # Save the TensorFlow SavedModel for Serving Predictions\r\n",
      "        print('tensorflow_saved_model_path {}'.format(tensorflow_saved_model_path))   \r\n",
      "        model.save(tensorflow_saved_model_path, save_format='tf')\r\n",
      "                \r\n",
      "        # Copy inference.py and requirements.txt to the code/ directory\r\n",
      "        #   Note: This is required for the SageMaker Endpoint to pick them up.\r\n",
      "        #         This appears to be hard-coded and must be called code/\r\n",
      "        inference_path = os.path.join(local_model_dir, 'code/')\r\n",
      "        print('Copying inference source files to {}'.format(inference_path))\r\n",
      "        os.makedirs(inference_path, exist_ok=True)               \r\n",
      "        os.system('cp inference.py {}'.format(inference_path))\r\n",
      "        print(glob(inference_path))        \r\n",
      "#        os.system('cp requirements.txt {}/code'.format(inference_path))\r\n",
      "        \r\n",
      "    if run_sample_predictions:\r\n",
      "        loaded_model = TFDistilBertForSequenceClassification.from_pretrained(transformer_fine_tuned_model_path,\r\n",
      "                                                                       id2label={\r\n",
      "                                                                        0: 1,\r\n",
      "                                                                        1: 2,\r\n",
      "                                                                        2: 3,\r\n",
      "                                                                        3: 4,\r\n",
      "                                                                        4: 5\r\n",
      "                                                                       },\r\n",
      "                                                                       label2id={\r\n",
      "                                                                        1: 0,\r\n",
      "                                                                        2: 1,\r\n",
      "                                                                        3: 2,\r\n",
      "                                                                        4: 3,\r\n",
      "                                                                        5: 4\r\n",
      "                                                                       })\r\n",
      "\r\n",
      "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "\r\n",
      "        if num_gpus >= 1:\r\n",
      "            inference_device = 0 # GPU 0\r\n",
      "        else:\r\n",
      "            inference_device = -1 # CPU\r\n",
      "        print('inference_device {}'.format(inference_device))\r\n",
      "\r\n",
      "        inference_pipeline = TextClassificationPipeline(model=loaded_model, \r\n",
      "                                                        tokenizer=tokenizer,\r\n",
      "                                                        framework='tf',\r\n",
      "                                                        device=inference_device)  \r\n",
      "\r\n",
      "        print(\"\"\"I loved it!  I will recommend this to everyone.\"\"\", inference_pipeline(\"\"\"I loved it!  I will recommend this to everyone.\"\"\"))\r\n",
      "        print(\"\"\"It's OK.\"\"\", inference_pipeline(\"\"\"It's OK.\"\"\"))\r\n",
      "        print(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\", inference_pipeline(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"))\r\n",
      "\r\n",
      "        import csv\r\n",
      "\r\n",
      "        df_test_reviews = pd.read_csv('./test_data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz', \r\n",
      "                                        delimiter='\\t', \r\n",
      "                                        quoting=csv.QUOTE_NONE,\r\n",
      "                                        compression='gzip')[['review_body', 'star_rating']]\r\n",
      "\r\n",
      "        df_test_reviews = df_test_reviews.sample(n=100)\r\n",
      "        df_test_reviews.shape\r\n",
      "        df_test_reviews.head()\r\n",
      "        \r\n",
      "        import pandas as pd\r\n",
      "\r\n",
      "        def predict(review_body):\r\n",
      "            prediction_map = inference_pipeline(review_body)\r\n",
      "            return prediction_map[0]['label']\r\n",
      "\r\n",
      "        y_test = df_test_reviews['review_body'].map(predict)\r\n",
      "        y_test\r\n",
      "        \r\n",
      "        y_actual = df_test_reviews['star_rating']\r\n",
      "        y_actual\r\n",
      "\r\n",
      "        from sklearn.metrics import classification_report\r\n",
      "        print(classification_report(y_true=y_test, y_pred=y_actual))\r\n",
      "        \r\n",
      "        from sklearn.metrics import accuracy_score\r\n",
      "        print('Accuracy: ', accuracy_score(y_true=y_test, y_pred=y_actual))\r\n",
      "        \r\n",
      "        import matplotlib.pyplot as plt\r\n",
      "        import pandas as pd\r\n",
      "\r\n",
      "        def plot_conf_mat(cm, classes, title, cmap = plt.cm.Greens):\r\n",
      "            print(cm)\r\n",
      "            plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
      "            plt.title(title)\r\n",
      "            plt.colorbar()\r\n",
      "            tick_marks = np.arange(len(classes))\r\n",
      "            plt.xticks(tick_marks, classes, rotation=45)\r\n",
      "            plt.yticks(tick_marks, classes)\r\n",
      "\r\n",
      "            fmt = 'd'\r\n",
      "            thresh = cm.max() / 2.\r\n",
      "            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
      "                plt.text(j, i, format(cm[i, j], fmt),\r\n",
      "                horizontalalignment=\"center\",\r\n",
      "                color=\"black\" if cm[i, j] > thresh else \"black\")\r\n",
      "\r\n",
      "                plt.tight_layout()\r\n",
      "                plt.ylabel('True label')\r\n",
      "                plt.xlabel('Predicted label')\r\n",
      "                \r\n",
      "        import itertools\r\n",
      "        import numpy as np\r\n",
      "        from sklearn.metrics import confusion_matrix\r\n",
      "        import matplotlib.pyplot as plt\r\n",
      "        #%matplotlib inline\r\n",
      "        #%config InlineBackend.figure_format='retina'\r\n",
      "\r\n",
      "        cm = confusion_matrix(y_true=y_test, y_pred=y_actual)\r\n",
      "\r\n",
      "        plt.figure()\r\n",
      "        fig, ax = plt.subplots(figsize=(10,5))\r\n",
      "        plot_conf_mat(cm, \r\n",
      "                      classes=['1', '2', '3', '4', '5'], \r\n",
      "                      title='Confusion Matrix')\r\n",
      "\r\n",
      "        # Save the confusion matrix        \r\n",
      "        plt.show()\r\n",
      "        \r\n",
      "        # Model Output \r\n",
      "        metrics_path = os.path.join(local_model_dir, 'metrics/')\r\n",
      "        os.makedirs(metrics_path, exist_ok=True)\r\n",
      "        plt.savefig('{}/confusion_matrix.png'.format(metrics_path))\r\n"
     ]
    }
   ],
   "source": [
    "!cat src/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Static Hyper-Parameters for Classification Layer\n",
    "First, retrieve `max_seq_length` from the prepare phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    max_seq_length\n",
    "    print('[OK]')\n",
    "except NameError:\n",
    "    print('+++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please run the notebooks in the PREPARE section before you continue.')\n",
    "    print('+++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.00000001\n",
    "validation_batch_size=128\n",
    "test_batch_size=128\n",
    "validation_steps=100\n",
    "test_steps=100\n",
    "train_instance_count=1\n",
    "#train_instance_type='ml.m5.4xlarge' #bur\n",
    "train_instance_type='ml.c5.4xlarge' #evt\n",
    "train_volume_size=1024\n",
    "use_xla=True\n",
    "use_amp=True\n",
    "enable_sagemaker_debugger=False\n",
    "enable_checkpointing=False\n",
    "enable_tensorboard=False\n",
    "input_mode='Pipe'\n",
    "run_validation=True\n",
    "run_test=True\n",
    "run_sample_predictions=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track the Optimizations Within our Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    experiment_name\n",
    "    print('[OK]')\n",
    "except NameError:\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please run the notebooks in the TRAIN section before you continue.')\n",
    "    print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon-Customer-Reviews-BERT-Experiment-1601145857\n"
     ]
    }
   ],
   "source": [
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r trial_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trial_name\n",
    "    print('[OK]')    \n",
    "except NameError:\n",
    "    print('+++++++++++++++++++++++++++++++')\n",
    "    print('[ERROR] Please run the notebooks in the previous TRAIN section before you continue.')\n",
    "    print('+++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial-1601145857\n"
     ]
    }
   ],
   "source": [
    "print(trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7fd6287a00f0>,trial_name='trial-1601145857',trial_arn='arn:aws:sagemaker:us-west-2:085964654406:experiment-trial/trial-1601145857',display_name='trial-1601145857',experiment_name='Amazon-Customer-Reviews-BERT-Experiment-1601145857',creation_time=datetime.datetime(2020, 9, 26, 18, 44, 17, 852000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 9, 26, 19, 19, 24, 267000, tzinfo=tzlocal()),last_modified_by={},response_metadata={'RequestId': 'fc5c4d8d-0235-465d-8be8-3261e8341f91', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'fc5c4d8d-0235-465d-8be8-3261e8341f91', 'content-type': 'application/x-amz-json-1.1', 'content-length': '326', 'date': 'Sat, 26 Sep 2020 19:43:47 GMT'}, 'RetryAttempts': 0})\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "timestamp = '{}'.format(int(time.time()))\n",
    "\n",
    "trial = Trial.load(trial_name=trial_name)\n",
    "print(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize trial component name TrialComponent-2020-09-26-194347-gwoe\n"
     ]
    }
   ],
   "source": [
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "tracker_optimize = Tracker.create(display_name='optimize-1', \n",
    "                                  sagemaker_boto_client=sm)\n",
    "\n",
    "optimize_trial_component_name = tracker_optimize.trial_component.trial_component_name\n",
    "print('Optimize trial component name {}'.format(optimize_trial_component_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach the `deploy` Trial Component and Tracker as a Component to the Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.add_trial_component(tracker_optimize.trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Dynamic Hyper-Parameter Ranges to Explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter\n",
    "from sagemaker.tuner import ContinuousParameter\n",
    "from sagemaker.tuner import CategoricalParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "                                                \n",
    "hyperparameter_ranges = {\n",
    "    'epochs': CategoricalParameter([1, 2]),\n",
    "    'learning_rate': ContinuousParameter(0.00001, 0.00005, scaling_type='Linear'),\n",
    "    'train_batch_size': CategoricalParameter([64, 128]),\n",
    "    'train_steps_per_epoch': CategoricalParameter([100, 200]),\n",
    "    'freeze_bert_layer': CategoricalParameter([True, False])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "     {'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='tf_bert_reviews.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       instance_count=train_instance_count,\n",
    "                       instance_type=train_instance_type,\n",
    "                       volume_size=train_volume_size,\n",
    "                       py_version='py3',\n",
    "                       framework_version='2.1.0',\n",
    "                       hyperparameters={\n",
    "                                        'epsilon': epsilon,\n",
    "                                        'validation_batch_size': validation_batch_size,\n",
    "                                        'test_batch_size': test_batch_size,                                                                                 \n",
    "                                        'validation_steps': validation_steps,\n",
    "                                        'test_steps': test_steps,\n",
    "                                        'use_xla': use_xla,\n",
    "                                        'use_amp': use_amp,\n",
    "                                        'max_seq_length': max_seq_length,\n",
    "                                        'enable_sagemaker_debugger': enable_sagemaker_debugger,                                        \n",
    "                                        'enable_checkpointing': enable_checkpointing,\n",
    "                                        'enable_tensorboard': enable_tensorboard,                                        \n",
    "                                        'run_validation': run_validation,\n",
    "                                        'run_test': run_test,\n",
    "                                        'run_sample_predictions': run_sample_predictions},\n",
    "                       input_mode=input_mode,\n",
    "                       metric_definitions=metrics_definitions,\n",
    "#                       max_run=7200 # max 2 hours * 60 minutes seconds per hour * 60 seconds per minute\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup HyperparameterTuner with Estimator and Hyper-Parameter Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'train:accuracy'\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_type='Maximize',\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metrics_definitions,\n",
    "    max_jobs=2,\n",
    "    max_parallel_jobs=2,\n",
    "    strategy='Random',\n",
    "    early_stopping_type='Off'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: tensorflow-training-200926-1943\n"
     ]
    }
   ],
   "source": [
    "tuner.fit(inputs={'train': s3_input_train_data, \n",
    "                  'validation': s3_input_validation_data,\n",
    "                  'test': s3_input_test_data\n",
    "          }, \n",
    "          include_cls_metadata=False,\n",
    "          wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Tuning Job Status\n",
    "Re-run this cell to track the status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2020, 9, 26, 19, 43, 51, 662000, tzinfo=tzlocal()),\n",
      " 'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-west-2:085964654406:hyper-parameter-tuning-job/tensorflow-training-200926-1943',\n",
      " 'HyperParameterTuningJobConfig': {'HyperParameterTuningJobObjective': {'MetricName': 'train:accuracy',\n",
      "                                                                        'Type': 'Maximize'},\n",
      "                                   'ParameterRanges': {'CategoricalParameterRanges': [{'Name': 'epochs',\n",
      "                                                                                       'Values': ['\"1\"',\n",
      "                                                                                                  '\"2\"']},\n",
      "                                                                                      {'Name': 'train_batch_size',\n",
      "                                                                                       'Values': ['\"64\"',\n",
      "                                                                                                  '\"128\"']},\n",
      "                                                                                      {'Name': 'train_steps_per_epoch',\n",
      "                                                                                       'Values': ['\"100\"',\n",
      "                                                                                                  '\"200\"']},\n",
      "                                                                                      {'Name': 'freeze_bert_layer',\n",
      "                                                                                       'Values': ['\"True\"',\n",
      "                                                                                                  '\"False\"']}],\n",
      "                                                       'ContinuousParameterRanges': [{'MaxValue': '5e-05',\n",
      "                                                                                      'MinValue': '1e-05',\n",
      "                                                                                      'Name': 'learning_rate',\n",
      "                                                                                      'ScalingType': 'Linear'}],\n",
      "                                                       'IntegerParameterRanges': []},\n",
      "                                   'ResourceLimits': {'MaxNumberOfTrainingJobs': 2,\n",
      "                                                      'MaxParallelTrainingJobs': 2},\n",
      "                                   'Strategy': 'Random',\n",
      "                                   'TrainingJobEarlyStoppingType': 'Off'},\n",
      " 'HyperParameterTuningJobName': 'tensorflow-training-200926-1943',\n",
      " 'HyperParameterTuningJobStatus': 'InProgress',\n",
      " 'LastModifiedTime': datetime.datetime(2020, 9, 26, 19, 43, 51, 662000, tzinfo=tzlocal()),\n",
      " 'ObjectiveStatusCounters': {'Failed': 0, 'Pending': 0, 'Succeeded': 0},\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '3768',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Sat, 26 Sep 2020 19:43:51 GMT',\n",
      "                                      'x-amzn-requestid': 'cbfd66e4-1552-470d-924b-da6a4a408d38'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'cbfd66e4-1552-470d-924b-da6a4a408d38',\n",
      "                      'RetryAttempts': 0},\n",
      " 'TrainingJobDefinition': {'AlgorithmSpecification': {'MetricDefinitions': [{'Name': 'train:loss',\n",
      "                                                                             'Regex': 'loss: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'train:accuracy',\n",
      "                                                                             'Regex': 'accuracy: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'validation:loss',\n",
      "                                                                             'Regex': 'val_loss: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'validation:accuracy',\n",
      "                                                                             'Regex': 'val_accuracy: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'ObjectiveMetric',\n",
      "                                                                             'Regex': 'accuracy: '\n",
      "                                                                                      '([0-9\\\\.]+)'}],\n",
      "                                                      'TrainingImage': '763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.1.0-cpu-py3',\n",
      "                                                      'TrainingInputMode': 'Pipe'},\n",
      "                           'EnableInterContainerTrafficEncryption': False,\n",
      "                           'EnableManagedSpotTraining': False,\n",
      "                           'EnableNetworkIsolation': False,\n",
      "                           'InputDataConfig': [{'ChannelName': 'train',\n",
      "                                                'DataSource': {'S3DataSource': {'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                                                'S3DataType': 'S3Prefix',\n",
      "                                                                                'S3Uri': 's3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-train'}}},\n",
      "                                               {'ChannelName': 'validation',\n",
      "                                                'DataSource': {'S3DataSource': {'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                                                'S3DataType': 'S3Prefix',\n",
      "                                                                                'S3Uri': 's3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-validation'}}},\n",
      "                                               {'ChannelName': 'test',\n",
      "                                                'DataSource': {'S3DataSource': {'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                                                'S3DataType': 'S3Prefix',\n",
      "                                                                                'S3Uri': 's3://sagemaker-us-west-2-085964654406/sagemaker-scikit-learn-2020-09-26-17-44-12-987/output/bert-test'}}}],\n",
      "                           'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-west-2-085964654406/'},\n",
      "                           'ResourceConfig': {'InstanceCount': 1,\n",
      "                                              'InstanceType': 'ml.c5.4xlarge',\n",
      "                                              'VolumeSizeInGB': 1024},\n",
      "                           'RoleArn': 'arn:aws:iam::085964654406:role/TeamRole',\n",
      "                           'StaticHyperParameters': {'_tuning_objective_metric': 'train:accuracy',\n",
      "                                                     'enable_checkpointing': 'false',\n",
      "                                                     'enable_sagemaker_debugger': 'false',\n",
      "                                                     'enable_tensorboard': 'false',\n",
      "                                                     'epsilon': '1e-08',\n",
      "                                                     'max_seq_length': '64',\n",
      "                                                     'model_dir': '\"s3://sagemaker-us-west-2-085964654406/tensorflow-training-2020-09-26-19-43-50-231/model\"',\n",
      "                                                     'run_sample_predictions': 'true',\n",
      "                                                     'run_test': 'true',\n",
      "                                                     'run_validation': 'true',\n",
      "                                                     'sagemaker_container_log_level': '20',\n",
      "                                                     'sagemaker_estimator_class_name': '\"TensorFlow\"',\n",
      "                                                     'sagemaker_estimator_module': '\"sagemaker.tensorflow.estimator\"',\n",
      "                                                     'sagemaker_job_name': '\"tensorflow-training-2020-09-26-19-43-50-231\"',\n",
      "                                                     'sagemaker_program': '\"tf_bert_reviews.py\"',\n",
      "                                                     'sagemaker_region': '\"us-west-2\"',\n",
      "                                                     'sagemaker_submit_directory': '\"s3://sagemaker-us-west-2-085964654406/tensorflow-training-2020-09-26-19-43-50-231/source/sourcedir.tar.gz\"',\n",
      "                                                     'test_batch_size': '128',\n",
      "                                                     'test_steps': '100',\n",
      "                                                     'use_amp': 'true',\n",
      "                                                     'use_xla': 'true',\n",
      "                                                     'validation_batch_size': '128',\n",
      "                                                     'validation_steps': '100'},\n",
      "                           'StoppingCondition': {'MaxRuntimeInSeconds': 86400}},\n",
      " 'TrainingJobStatusCounters': {'Completed': 0,\n",
      "                               'InProgress': 0,\n",
      "                               'NonRetryableError': 0,\n",
      "                               'RetryableError': 0,\n",
      "                               'Stopped': 0}}\n",
      "Not yet complete, but 0 jobs have completed.\n",
      "No training jobs have reported results yet.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "\n",
    "job_description = sm.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "status = job_description['HyperParameterTuningJobStatus']\n",
    "\n",
    "pprint(job_description)\n",
    "\n",
    "if status != 'Completed':\n",
    "    job_count = job_description['TrainingJobStatusCounters']['Completed']\n",
    "    print('Not yet complete, but {} jobs have completed.'.format(job_count))\n",
    "    \n",
    "    if job_description.get('BestTrainingJob', None):\n",
    "        print(\"Best candidate:\")\n",
    "        pprint(job_description['BestTrainingJob']['TrainingJobName'])\n",
    "        pprint(job_description['BestTrainingJob']['FinalHyperParameterTuningJobObjectiveMetric'])\n",
    "    else:\n",
    "        print(\"No training jobs have reported results yet.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-west-2#/hyper-tuning-jobs/tensorflow-training-200926-1943\">Hyper-Parameter Tuning Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "    \n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/hyper-tuning-jobs/{}\">Hyper-Parameter Tuning Job</a></b>'.format(region, tuning_job_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait for the ^^ Tuning Job ^^ to Complete Above_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................................................................................................................................................................................................................................................!\n",
      "CPU times: user 1.24 s, sys: 98.5 ms, total: 1.34 s\n",
      "Wall time: 29min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [INFO] _Feel free to continue to the next workshop section while this notebook is running._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Tuning Job\n",
    "### _Note:  This will fail at first.  Please wait about 15-30 seconds and re-run._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 11)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "hp_results = HyperparameterTuningJobAnalytics(\n",
    "    sagemaker_session=sess, \n",
    "    hyperparameter_tuning_job_name=tuning_job_name\n",
    ")\n",
    "\n",
    "df_results = hp_results.dataframe()\n",
    "df_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>freeze_bert_layer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>train_steps_per_epoch</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"1\"</td>\n",
       "      <td>\"False\"</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>\"128\"</td>\n",
       "      <td>\"200\"</td>\n",
       "      <td>tensorflow-training-200926-1943-002-75de2dd3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>2020-09-26 19:45:42+00:00</td>\n",
       "      <td>2020-09-26 20:11:55+00:00</td>\n",
       "      <td>1573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"1\"</td>\n",
       "      <td>\"True\"</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>\"128\"</td>\n",
       "      <td>\"100\"</td>\n",
       "      <td>tensorflow-training-200926-1943-001-1a7cfa05</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.1611</td>\n",
       "      <td>2020-09-26 19:45:54+00:00</td>\n",
       "      <td>2020-09-26 20:03:04+00:00</td>\n",
       "      <td>1030.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  epochs freeze_bert_layer  learning_rate train_batch_size  \\\n",
       "0    \"1\"           \"False\"       0.000014            \"128\"   \n",
       "1    \"1\"            \"True\"       0.000013            \"128\"   \n",
       "\n",
       "  train_steps_per_epoch                               TrainingJobName  \\\n",
       "0                 \"200\"  tensorflow-training-200926-1943-002-75de2dd3   \n",
       "1                 \"100\"  tensorflow-training-200926-1943-001-1a7cfa05   \n",
       "\n",
       "  TrainingJobStatus  FinalObjectiveValue         TrainingStartTime  \\\n",
       "0         Completed               0.4674 2020-09-26 19:45:42+00:00   \n",
       "1         Completed               0.1611 2020-09-26 19:45:54+00:00   \n",
       "\n",
       "            TrainingEndTime  TrainingElapsedTimeSeconds  \n",
       "0 2020-09-26 20:11:55+00:00                      1573.0  \n",
       "1 2020-09-26 20:03:04+00:00                      1030.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values('FinalObjectiveValue', ascending=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Best Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>freeze_bert_layer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>train_steps_per_epoch</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"1\"</td>\n",
       "      <td>\"False\"</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>\"128\"</td>\n",
       "      <td>\"200\"</td>\n",
       "      <td>tensorflow-training-200926-1943-002-75de2dd3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>2020-09-26 19:45:42+00:00</td>\n",
       "      <td>2020-09-26 20:11:55+00:00</td>\n",
       "      <td>1573.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  epochs freeze_bert_layer  learning_rate train_batch_size  \\\n",
       "0    \"1\"           \"False\"       0.000014            \"128\"   \n",
       "\n",
       "  train_steps_per_epoch                               TrainingJobName  \\\n",
       "0                 \"200\"  tensorflow-training-200926-1943-002-75de2dd3   \n",
       "\n",
       "  TrainingJobStatus  FinalObjectiveValue         TrainingStartTime  \\\n",
       "0         Completed               0.4674 2020-09-26 19:45:42+00:00   \n",
       "\n",
       "            TrainingEndTime  TrainingElapsedTimeSeconds  \n",
       "0 2020-09-26 20:11:55+00:00                      1573.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values('FinalObjectiveValue', ascending=0).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log the Best Hyper-Parameter and Objective Metric in the Experiment\n",
    "\n",
    "Logging `learning_rate` parameter and `accuracy` metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.000014\n",
      "Name: learning_rate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "best_learning_rate = df_results.sort_values('FinalObjectiveValue', ascending=0).head(1)['learning_rate']\n",
    "print(best_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.4674\n",
      "Name: FinalObjectiveValue, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = df_results.sort_values('FinalObjectiveValue', ascending=0).head(1)['FinalObjectiveValue']\n",
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialComponent(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7fd628d8b0f0>,trial_component_name='TrialComponent-2020-09-26-194347-gwoe',display_name='optimize-1',tags=None,trial_component_arn='arn:aws:sagemaker:us-west-2:085964654406:experiment-trial-component/trialcomponent-2020-09-26-194347-gwoe',response_metadata={'RequestId': 'd6290de2-008c-461c-b6f2-7d264c812366', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'd6290de2-008c-461c-b6f2-7d264c812366', 'content-type': 'application/x-amz-json-1.1', 'content-length': '129', 'date': 'Sat, 26 Sep 2020 20:13:21 GMT'}, 'RetryAttempts': 0},parameters={'learning_rate': 1.432303027860436e-05},input_artifacts={},output_artifacts={})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker_optimize.log_parameters({\n",
    "    'learning_rate': float(best_learning_rate)\n",
    "})\n",
    "\n",
    "# must save after logging\n",
    "tracker_optimize.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialComponent(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7fd628d8b0f0>,trial_component_name='TrialComponent-2020-09-26-194347-gwoe',display_name='optimize-1',tags=None,trial_component_arn='arn:aws:sagemaker:us-west-2:085964654406:experiment-trial-component/trialcomponent-2020-09-26-194347-gwoe',response_metadata={'RequestId': '448d460f-bf2d-488e-ae90-61bd81d78348', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '448d460f-bf2d-488e-ae90-61bd81d78348', 'content-type': 'application/x-amz-json-1.1', 'content-length': '129', 'date': 'Sat, 26 Sep 2020 20:13:21 GMT'}, 'RetryAttempts': 0},parameters={'learning_rate': 1.432303027860436e-05},input_artifacts={},output_artifacts={})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker_optimize.log_metric('accuracy', float(best_accuracy))\n",
    "\n",
    "# must save after logging\n",
    "tracker_optimize.trial_component.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Experiment Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=sess,\n",
    "    experiment_name=experiment_name,\n",
    "    metric_names=['validation:accuracy'],\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\",\n",
    ")\n",
    "\n",
    "lineage_df = lineage_table.dataframe()\n",
    "lineage_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>balance_dataset</th>\n",
       "      <th>max_seq_length</th>\n",
       "      <th>test_split_percentage</th>\n",
       "      <th>train_split_percentage</th>\n",
       "      <th>validation_split_percentage</th>\n",
       "      <th>raw_data_s3_uri - MediaType</th>\n",
       "      <th>raw_data_s3_uri - Value</th>\n",
       "      <th>test_data_s3_uri - MediaType</th>\n",
       "      <th>...</th>\n",
       "      <th>train - MediaType</th>\n",
       "      <th>train - Value</th>\n",
       "      <th>validation - MediaType</th>\n",
       "      <th>validation - Value</th>\n",
       "      <th>SageMaker.Checkpoints - MediaType</th>\n",
       "      <th>SageMaker.Checkpoints - Value</th>\n",
       "      <th>SageMaker.DebugHookOutput - MediaType</th>\n",
       "      <th>SageMaker.DebugHookOutput - Value</th>\n",
       "      <th>SageMaker.ModelArtifact - MediaType</th>\n",
       "      <th>SageMaker.ModelArtifact - Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TrialComponent-2020-09-26-184417-oahc</td>\n",
       "      <td>prepare</td>\n",
       "      <td>True</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>s3/uri</td>\n",
       "      <td>s3://sagemaker-us-west-2-085964654406/amazon-r...</td>\n",
       "      <td>s3/uri</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensorflow-training-2020-09-26-18-44-25-975-aw...</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-west-2-085964654406/sagemake...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-west-2-085964654406/sagemake...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-west-2-085964654406/checkpoi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-west-2-085964654406/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-west-2-085964654406/tensorfl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TrialComponent-2020-09-26-194347-gwoe</td>\n",
       "      <td>optimize-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName DisplayName  \\\n",
       "0              TrialComponent-2020-09-26-184417-oahc     prepare   \n",
       "1  tensorflow-training-2020-09-26-18-44-25-975-aw...       train   \n",
       "2              TrialComponent-2020-09-26-194347-gwoe  optimize-1   \n",
       "\n",
       "  balance_dataset  max_seq_length  test_split_percentage  \\\n",
       "0            True            64.0                   0.05   \n",
       "1             NaN            64.0                    NaN   \n",
       "2             NaN             NaN                    NaN   \n",
       "\n",
       "   train_split_percentage  validation_split_percentage  \\\n",
       "0                     0.9                         0.05   \n",
       "1                     NaN                          NaN   \n",
       "2                     NaN                          NaN   \n",
       "\n",
       "  raw_data_s3_uri - MediaType  \\\n",
       "0                      s3/uri   \n",
       "1                         NaN   \n",
       "2                         NaN   \n",
       "\n",
       "                             raw_data_s3_uri - Value  \\\n",
       "0  s3://sagemaker-us-west-2-085964654406/amazon-r...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "\n",
       "  test_data_s3_uri - MediaType  ... train - MediaType  \\\n",
       "0                       s3/uri  ...               NaN   \n",
       "1                          NaN  ...               NaN   \n",
       "2                          NaN  ...               NaN   \n",
       "\n",
       "                                       train - Value validation - MediaType  \\\n",
       "0                                                NaN                    NaN   \n",
       "1  s3://sagemaker-us-west-2-085964654406/sagemake...                    NaN   \n",
       "2                                                NaN                    NaN   \n",
       "\n",
       "                                  validation - Value  \\\n",
       "0                                                NaN   \n",
       "1  s3://sagemaker-us-west-2-085964654406/sagemake...   \n",
       "2                                                NaN   \n",
       "\n",
       "  SageMaker.Checkpoints - MediaType  \\\n",
       "0                               NaN   \n",
       "1                               NaN   \n",
       "2                               NaN   \n",
       "\n",
       "                       SageMaker.Checkpoints - Value  \\\n",
       "0                                                NaN   \n",
       "1  s3://sagemaker-us-west-2-085964654406/checkpoi...   \n",
       "2                                                NaN   \n",
       "\n",
       "  SageMaker.DebugHookOutput - MediaType  \\\n",
       "0                                   NaN   \n",
       "1                                   NaN   \n",
       "2                                   NaN   \n",
       "\n",
       "        SageMaker.DebugHookOutput - Value SageMaker.ModelArtifact - MediaType  \\\n",
       "0                                     NaN                                 NaN   \n",
       "1  s3://sagemaker-us-west-2-085964654406/                                 NaN   \n",
       "2                                     NaN                                 NaN   \n",
       "\n",
       "                     SageMaker.ModelArtifact - Value  \n",
       "0                                                NaN  \n",
       "1  s3://sagemaker-us-west-2-085964654406/tensorfl...  \n",
       "2                                                NaN  \n",
       "\n",
       "[3 rows x 64 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass `tuning_job_name` to the Next Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-training-200926-1943\n"
     ]
    }
   ],
   "source": [
    "print(tuning_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tuning_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store tuning_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "auto_ml_job_name                                      -> 'automl-dm-26-16-00-25'\n",
      "autopilot_endpoint_name                               -> 'automl-dm-ep-26-16-21-49'\n",
      "autopilot_train_s3_uri                                -> 's3://sagemaker-us-west-2-085964654406/data/amazon\n",
      "balance_dataset                                       -> True\n",
      "experiment_name                                       -> 'Amazon-Customer-Reviews-BERT-Experiment-160114585\n",
      "ingest_create_athena_db_passed                        -> True\n",
      "ingest_create_athena_table_parquet_passed             -> True\n",
      "ingest_create_athena_table_tsv_passed                 -> True\n",
      "max_seq_length                                        -> 64\n",
      "prepare_trial_component_name                          -> 'TrialComponent-2020-09-26-184417-oahc'\n",
      "processed_test_data_s3_uri                            -> 's3://sagemaker-us-west-2-085964654406/sagemaker-s\n",
      "processed_train_data_s3_uri                           -> 's3://sagemaker-us-west-2-085964654406/sagemaker-s\n",
      "processed_validation_data_s3_uri                      -> 's3://sagemaker-us-west-2-085964654406/sagemaker-s\n",
      "raw_input_data_s3_uri                                 -> 's3://sagemaker-us-west-2-085964654406/amazon-revi\n",
      "s3_private_path_tsv                                   -> 's3://sagemaker-us-west-2-085964654406/amazon-revi\n",
      "s3_public_path_tsv                                    -> 's3://amazon-reviews-pds/tsv'\n",
      "setup_dependencies_passed                             -> True\n",
      "setup_iam_roles_passed                                -> True\n",
      "setup_instance_check_passed                           -> True\n",
      "setup_s3_bucket_passed                                -> True\n",
      "test_split_percentage                                 -> 0.05\n",
      "train_split_percentage                                -> 0.9\n",
      "training_job_debugger_artifacts_path                  -> 's3://sagemaker-us-west-2-085964654406/tensorflow-\n",
      "training_job_name                                     -> 'tensorflow-training-2020-09-26-18-44-25-975'\n",
      "transformer_pytorch_model_name                        -> 'pytorch_model.bin'\n",
      "transformer_pytorch_model_s3_uri                      -> 's3://sagemaker-us-west-2-085964654406/models/tran\n",
      "trial_name                                            -> 'trial-1601145857'\n",
      "tuning_job_name                                       -> 'tensorflow-training-200926-1943'\n",
      "validation_split_percentage                           -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
